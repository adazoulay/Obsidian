# Raw WebGPU

[https://alain.xyz/blog/raw-webgpu](https://alain.xyz/blog/raw-webgpu)

*An overview on how to write a WebGPU application. Learn what key data structures and types are needed to draw in WebGPU.*

# Setup

```bash
# ğŸ‘ Clone the repo
git clone https://github.com/alaingalvan/webgpu-seed

# ğŸ’¿ go inside the folder
cd webgpu-seed

# ğŸ”¨ Start building the project
npm start
```

## Project Layout

As your project becomes more complex, you'll want to separate files and organize your application to something more akin to a game or renderer

```bash
â”œâ”€ ğŸ“‚ node_modules/   # ğŸ‘¶ Dependencies
â”‚  â”œâ”€ ğŸ“ gl-matrix      # â• Linear Algebra
â”‚  â””â”€ ğŸ“ ...            # ğŸ•š Other Dependencies (TypeScript, Webpack, etc.)
â”œâ”€ ğŸ“‚ src/            # ğŸŒŸ Source Files
â”‚  â”œâ”€ ğŸ“„ renderer.ts    # ğŸ”º Triangle Renderer
â”‚  â””â”€ ğŸ“„ main.ts        # ğŸ Application Main
â”œâ”€ ğŸ“„ .gitignore      # ğŸ‘ï¸ Ignore Certain Files in Git Repo
â”œâ”€ ğŸ“„ package.json    # ğŸ“¦ Node Package File
â”œâ”€ ğŸ“„ [[]]      # âš–ï¸ Your License (Unlicense)
â””â”€ [[]]        # ğŸ“– Read Me!
```

### Dependencies

- [gl-matrix](https://github.com/toji/gl-matrix)Â - A JavaScript library that allows users to writeÂ `glsl`Â like JavaScript code, with types for vectors, matrices, etc.
    - Not in use in this sample, useful for programming more advanced topics such as camera matrices
- [TypeScript](https://github.com/microsoft/typescript)Â - JavaScript with types, makes it significantly easier to program web apps with instant autocomplete and type checking
- [Webpack](https://github.com/webpack/webpack)Â - A JavaScript compilation tool to build minified outputs and test our apps faster

## Overview

1. **Initialize the API**Â - Check ifÂ `navigator.gpu`Â exists, and if it does, request aÂ `GPUAdapter`, then request aÂ `GPUDevice`, and get that device's defaultÂ `GPUQueue`.
2. **Setup Frame Backings**Â - create aÂ `GPUCanvasContext`Â and configure it to receive aÂ `GPUTexture`Â for the current frame, as well as any other attachments you might need (such as a depth-stencil texture, etc.). CreateÂ `GPUTextureView`s for those textures.
3. **Initialize Resources**Â - Create your Vertex and IndexÂ `GPUBuffer`s, load your WebGPU Shading Language (WGSL) shaders asÂ `GPUShaderModule`s, create yourÂ `GPURenderPipeline`Â by describing every stage of the graphics pipeline. Finally, build yourÂ `GPUCommandEncoder`Â with what what render passes you intend to run, then aÂ `GPURenderPassEncoder`Â with all the draw calls you intend to execute for that render pass.
4. **Render**Â - Submit yourÂ `GPUCommandEncoder`Â by callingÂ `.finish()`, and submitting that to yourÂ `GPUQueue`. Refresh the canvas context by callingÂ `requestAnimationFrame`.
5. **Destroy**Â - Destroy any data structures after you're done using the API.

## Start

To access the WebGPU API, you need to see if there exists aÂ `gpu`Â object in the globalÂ `navigator`

```jsx
// ğŸ­ Entry to WebGPU
const entry: GPU = navigator.gpu;
if (!entry) {
    throw new Error('WebGPU is not supported on this browser.');
}
```

### Adapter

AnÂ **Adapter**Â describes the physical properties of a given GPU, such as its name, extensions, and device limits

```jsx
// âœ‹ Declare adapter handle
let adapter: GPUAdapter = null;

// ğŸ™ Inside an async function...

// ğŸ”Œ Physical Device Adapter
adapter = await entry.requestAdapter();
```

### Device

AÂ **Device**Â is how you access theÂ *core*Â of the WebGPU API, and will allow you to create the data structures you'll need

```jsx
// âœ‹ Declare device handle
let device: GPUDevice = null;

// ğŸ™ Inside an async function...

// ğŸ’» Logical Device
device = await adapter.requestDevice();
```

### Queue

AÂ **Queue**Â allows you to send work asynchronously to the GPU. As of the writing of this post, you can only access a single default queue from a givenÂ `GPUDevice`

```jsx
// âœ‹ Declare queue handle
let queue: GPUQueue = null;

// ğŸ“¦ Queue
queue = device.queue;
```

## Frame Backings

### Canvas Context

In order to see what you're drawing, you'll need anÂ `HTMLCanvasElement`Â and to setup aÂ **Canvas Context**Â from that canvas

- A Canvas Context manages a series of textures you'll use to present your final render output to yourÂ `<canvas>`Â element

```jsx
// âœ‹ Declare context handle
const context: GPUCanvasContext = null;

// âšª Create Context
context = canvas.getContext('webgpu');

// â›“ï¸ Configure Context
const canvasConfig: GPUCanvasConfiguration = {
    device: this.device,
    format: 'bgra8unorm',
    usage: GPUTextureUsage.RENDER_ATTACHMENT | GPUTextureUsage.COPY_SRC,
    alphaMode: 'opaque'
};

context.configure(canvasConfig);
```

### Frame Buffer Attachments

When executing different passes of your rendering system, you'll need output textures to write to

- Textures could be for depth testing or shadows, or attachments for various aspects of a deferred renderer such as view space normals, PBR reflectivity/roughness, etc

Frame buffers attachments are references to texture views, which you'll see later when we write our rendering logic

```jsx
// âœ‹ Declare attachment handles
let depthTexture: GPUTexture = null;
let depthTextureView: GPUTextureView = null;

// ğŸ¤” Create Depth Backing
const depthTextureDesc: GPUTextureDescriptor = {
    size: [canvas.width, canvas.height, 1],
    dimension: '2d',
    format: 'depth24plus-stencil8',
    usage: GPUTextureUsage.RENDER_ATTACHMENT | GPUTextureUsage.COPY_SRC
};

depthTexture = device.createTexture(depthTextureDesc);
depthTextureView = depthTexture.createView();

// âœ‹ Declare canvas context image handles
let colorTexture: GPUTexture = null;
let colorTextureView: GPUTextureView = null;

colorTexture = context.getCurrentTexture();
colorTextureView = colorTexture.createView();
```

## Initialize Resources

### Vertex & Index Buffers

![[/Screen Shot 2023-05-29 at 5.40.38 PM.png]]

AÂ **Buffer**Â is an array of data, such as a mesh's positional data, color data, index data, etc.

When rendering triangles with a raster basedÂ *graphics pipeline*, you'll need:

- 1 or more buffers of vertex data
    - (commonly referred to asÂ *Vertex Buffer Objects*Â orÂ *VBO*s)
- 1 buffer of the indices that correspond with each triangle vertex that you intend to draw
    - (otherwise known as anÂ *Index Buffer Object*Â orÂ *IBO*).

```jsx
// ğŸ“ˆ Position Vertex Buffer Data
const positions = new Float32Array([
    1.0, -1.0, 0.0,
	 -1.0, -1.0, 0.0,
	  0.0, 1.0, 0.0
]);

// ğŸ¨ Color Vertex Buffer Data
const colors = new Float32Array([
    1.0, 0.0, 0.0, // ğŸ”´
    0.0, 1.0, 0.0, // ğŸŸ¢
		0.0, 0.0, 1.0 // ğŸ”µ
]);

// ğŸ“‡ Index Buffer Data
const indices = new Uint16Array([0, 1, 2]);

// âœ‹ Declare buffer handles
let positionBuffer: GPUBuffer = null;
let colorBuffer: GPUBuffer = null;
let indexBuffer: GPUBuffer = null;

// ğŸ‘‹ Helper function for creating GPUBuffer(s) out of Typed Arrays
const createBuffer = (arr: Float32Array | Uint16Array, usage: number) => {
    // ğŸ“ Align to 4 bytes (thanks @chrimsonite)
    let desc = {
        size: (arr.byteLength + 3) & ~3,
        usage,
        mappedAtCreation: true
    };
    let buffer = device.createBuffer(desc);

    const writeArray =
        arr instanceof Uint16Array
            ? new Uint16Array(buffer.getMappedRange())
            : new Float32Array(buffer.getMappedRange());
    writeArray.set(arr);
    buffer.unmap();
    return buffer;
};

positionBuffer = createBuffer(positions, GPUBufferUsage.VERTEX);
colorBuffer = createBuffer(colors, GPUBufferUsage.VERTEX);
indexBuffer = createBuffer(indices, GPUBufferUsage.INDEX);
```

### Shaders

With WebGPU comes a new shader language:Â **WebGPU Shading Language (WGSL)**:

WebGPU Shading Language is similar to other languages like Rust with JavaScript style decorators such asÂ `@location(0)`

Rust style code formatting withÂ `snake_case`Â functions/members,Â `CamelCase`Â structs, and functions followingÂ `fn my_func() -> i32`

Here's the vertex shader source:

```rust
struct VSOut {
    @builtin(position) nds_position: vec4<f32>,
    @location(0) color: vec3<f32>,
};

@vertex
fn main(@location(0) in_pos: vec3<f32>,
        @location(1) in_color: vec3<f32>) -> VSOut {
    var vs_out: VSOut;
    vs_out.nds_position = vec4<f32>(in_pos, 1.0);
    vs_out.color = inColor;
    return vsOut;
}
```

Here's the fragment shader source:

```rust
@fragment
fn main(@location(0) in_color: vec3<f32>) -> @location(0) vec4<f32> {
    return vec4<f32>(in_color, 1.0);
}
```

### Shader Modules

**Shader Module**s areÂ *plain text WGSL files*Â that execute on the GPU when executing a given pipeline

- Can be imported to use in createShaderModule declaration:

```jsx
// ğŸ“„ Import or declare in line your WGSL code:
import vertShaderCode from './shaders/triangle.vert.wgsl';
import fragShaderCode from './shaders/triangle.frag.wgsl';

// âœ‹ Declare shader module handles
let vertModule: GPUShaderModule = null;
let fragModule: GPUShaderModule = null;

const vsmDesc = { code: vertShaderCode };
vertModule = device.createShaderModule(vsmDesc);

const fsmDesc = { code: fragShaderCode };
fragModule = device.createShaderModule(fsmDesc);
```

### Uniform Buffer

![[/Screen Shot 2023-05-29 at 5.57.18 PM.png]]

You'll often times need to feed data directly to your shader modules, and to do this you'll need to specify a **uniform**

- A **uniform** is a value from a buffer that is the same for every invocation, available at all shader stages in a pipeline

In order to create aÂ **Uniform Buffer**Â in your shader, declare the following prior to your main function:

```rust
struct UBO {
  modelViewProj: mat4x4<f32>,
  primaryColor: vec4<f32>,
  accentColor: vec4<f32>
};

@group(0) @binding(0)
var<uniform> uniforms: UBO;

// â— Then in your Vertex Shader's main file,
// replace the 4th to last line with:
vsOut.Position = uniforms.modelViewProj * vec4<f32>(inPos, 1.0);
```

Then in your JavaScript code, create a uniform buffer as you would with an index/vertex buffer.

```jsx
// ğŸ‘” Uniform Data
const uniformData = new Float32Array([

    // â™Ÿï¸ ModelViewProjection Matrix (Identity)
    1.0, 0.0, 0.0, 0.0
    0.0, 1.0, 0.0, 0.0
    0.0, 0.0, 1.0, 0.0
    0.0, 0.0, 0.0, 1.0

    // ğŸ”´ Primary Color
    0.9, 0.1, 0.3, 1.0

    // ğŸŸ£ Accent Color
    0.8, 0.2, 0.8, 1.0
]);

// âœ‹ Declare buffer handles
let uniformBuffer: GPUBuffer = null;

uniformBuffer = createBuffer(uniformData, GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST);
```

### Pipeline Layout

![[/Screen Shot 2023-05-29 at 6.11.29 PM.png]]

Once you have a uniform, you can create aÂ **Pipeline Layout**Â to describe where that uniform will be when executing aÂ *Graphics Pipeline*

You have 2 options here

1. You can either let WebGPU create your pipeline layout for you
2. Get it from a pipeline that's already been created:

```jsx
let bindGroupLayout: GPUBindGroupLayout = null;
let uniformBindGroup: GPUBindGroup = null;

// ğŸ‘¨â€ğŸ”§ Create your graphics pipeline...

// ğŸ§™â€â™‚ï¸ Then get your implicit pipeline layout:
bindGroupLayout = pipeline.getBindGroupLayout(0);

// ğŸ—„ï¸ Bind Group
// âœ This would be used when *encoding commands*
uniformBindGroup = device.createBindGroup({
    layout: bindGroupLayout,
    entries: [
        {
            binding: 0,
            resource: {
                buffer: uniformBuffer
            }
        }
    ]
});
```

Or if you know the layout in advance, you can describe it yourself and use it during pipeline creation:

```jsx
// âœ‹ Declare handles
let uniformBindGroupLayout: GPUBindGroupLayout = null;
let uniformBindGroup: GPUBindGroup = null;
let layout: GPUPipelineLayout = null;

// ğŸ“ Bind Group Layout
uniformBindGroupLayout = device.createBindGroupLayout({
    entries: [
        {
            binding: 0,
            visibility: GPUShaderStage.VERTEX,
            buffer: {}
        }
    ]
});

// ğŸ—„ï¸ Bind Group
// âœ This would be used when *encoding commands*
uniformBindGroup = device.createBindGroup({
    layout: uniformBindGroupLayout,
    entries: [
        {
            binding: 0,
            resource: {
                buffer: uniformBuffer
            }
        }
    ]
});

// ğŸ—‚ï¸ Pipeline Layout
// ğŸ‘©â€ğŸ”§ This would be used as a member of a GPUPipelineDescriptor when *creating a pipeline*
const pipelineLayoutDesc = { bindGroupLayouts: [uniformBindGroupLayout] };
layout = device.createPipelineLayout(pipelineLayoutDesc);
```

When encoding commands, you can use this uniform withÂ `setBindGroup`:

```jsx
// âœ Later when you're encoding commands:
passEncoder.setBindGroup(0, uniformBindGroup);
```

### Graphics Pipeline

![[/Screen Shot 2023-05-29 at 6.25.03 PM.png]]

AÂ **Graphics Pipeline**Â describes all the data that's to be fed into the execution of a raster based graphics pipeline. This includes:

- ğŸ”£Â **Input Assembly**Â - What does each vertex look like? Which attributes are where, and how do they align in memory?
- ğŸ–ï¸Â **Shader Modules**Â - What shader modules will you be using when executing this graphics pipeline?
- âœï¸Â **Depth/Stencil State**Â - Should you perform depth testing? If so, what function should you use to test depth?
- ğŸ¥Â **Blend State**Â - How should colors be blended between the previously written color and current one?
- ğŸ”ºÂ **Rasterization**Â - How does the rasterizer behave when executing this graphics pipeline? Does it cull faces? Which direction should the face be culled?
- ğŸ’¾Â **Uniform Data**Â - What kind of uniform data should your shaders expect? In WebGPU this is done by describing aÂ **Pipeline Layout**.

WebGPU features smart defaults for the graphics pipeline state, so most of the time you won't need to even set parts of it:

```jsx
// âœ‹ Declare pipeline handle
let pipeline: GPURenderPipeline = null;

// âš—ï¸ Graphics Pipeline

// ğŸ”£ Input Assembly
const positionAttribDesc: GPUVertexAttribute = {
    shaderLocation: 0, // @location(0)
    offset: 0,
    format: 'float32x3'
};
const colorAttribDesc: GPUVertexAttribute = {
    shaderLocation: 1, // @location(1)
    offset: 0,
    format: 'float32x3'
};
const positionBufferDesc: GPUVertexBufferLayout = {
    attributes: [positionAttribDesc],
    arrayStride: 4 * 3, // sizeof(float) * 3
    stepMode: 'vertex'
};
const colorBufferDesc: GPUVertexBufferLayout = {
    attributes: [colorAttribDesc],
    arrayStride: 4 * 3, // sizeof(float) * 3
    stepMode: 'vertex'
};

// ğŸŒ‘ Depth
const depthStencil: GPUDepthStencilState = {
    depthWriteEnabled: true,
    depthCompare: 'less',
    format: 'depth24plus-stencil8'
};

// ğŸ¦„ Uniform Data
const pipelineLayoutDesc = { bindGroupLayouts: [] };
const layout = device.createPipelineLayout(pipelineLayoutDesc);

// ğŸ­ Shader Stages
const vertex: GPUVertexState = {
    module: vertModule,
    entryPoint: 'main',
    buffers: [positionBufferDesc, colorBufferDesc]
};

// ğŸŒ€ Color/Blend State
const colorState: GPUColorTargetState = {
    format: 'bgra8unorm'
};

const fragment: GPUFragmentState = {
    module: fragModule,
    entryPoint: 'main',
    targets: [colorState]
};

// ğŸŸ¨ Rasterization
const primitive: GPUPrimitiveState = {
    frontFace: 'cw',
    cullMode: 'none',
    topology: 'triangle-list'
};

const pipelineDesc: GPURenderPipelineDescriptor = {
    layout,
    vertex,
    fragment,
    primitive,
    depthStencil
};

pipeline = device.createRenderPipeline(pipelineDesc);
```

### Command Encoder

![[/Screen Shot 2023-05-29 at 6.28.07 PM.png]]

**Command Encoder**s encode all the draw commands you intend to execute in groups ofÂ **Render Pass Encoders**

Once you've finished encoding commands, you'll receive aÂ **Command Buffer**Â that you could submit to your queue

- In that sense a command buffer is analogous to aÂ *callback*Â that executes draw functions on the GPU once it's submitted to the queue

```jsx
// âœ‹ Declare command handles
let commandEncoder: GPUCommandEncoder = null;
let passEncoder: GPURenderPassEncoder = null;

// âœï¸ Write commands to send to the GPU
const encodeCommands = () => {
    let colorAttachment: GPURenderPassColorAttachment = {
        view: this.colorTextureView,
        clearValue: { r: 0, g: 0, b: 0, a: 1 },
        loadOp: 'clear',
        storeOp: 'store'
    };

    const depthAttachment: GPURenderPassDepthStencilAttachment = {
        view: this.depthTextureView,
        depthClearValue: 1,
        depthLoadOp: 'clear',
        depthStoreOp: 'store',
        stencilClearValue: 0,
        stencilLoadOp: 'clear',
        stencilStoreOp: 'store'
    };

    const renderPassDesc: GPURenderPassDescriptor = {
        colorAttachments: [colorAttachment],
        depthStencilAttachment: depthAttachment
    };

    commandEncoder = device.createCommandEncoder();

    // ğŸ–Œï¸ Encode drawing commands
    passEncoder = commandEncoder.beginRenderPass(renderPassDesc);
    passEncoder.setPipeline(pipeline);
    passEncoder.setViewport(0, 0, canvas.width, canvas.height, 0, 1);
    passEncoder.setScissorRect(0, 0, canvas.width, canvas.height);
    passEncoder.setVertexBuffer(0, positionBuffer);
    passEncoder.setVertexBuffer(1, colorBuffer);
    passEncoder.setIndexBuffer(indexBuffer, 'uint16');
    passEncoder.drawIndexed(3);
    passEncoder.endPass();

    queue.submit([commandEncoder.finish()]);
};
```

### Render

**Rendering**Â in WebGPU is a simple matter of updating any uniforms you intend to update, getting the next attachments from your context, submitting your command encoders to be executed, and using theÂ `requestAnimationFrame`Â callback to do all of that again

```jsx
const render = () => {
    // â­ Acquire next image from context
    colorTexture = context.getCurrentTexture();
    colorTextureView = colorTexture.createView();

    // ğŸ“¦ Write and submit commands to queue
    encodeCommands();

    // â¿ Refresh canvas
    requestAnimationFrame(render);
};
```