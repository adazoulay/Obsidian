# Shrinking .wasm size

## Why are about `.wasm` file size?

When serving a `.wasm` file over the network, the smaller it is, the faster the client can download it.

WebAssembly is typically served to users gzip'd so you'll want to be sure to compare differences in gzip'd size for transfer times over the wire

- wasm is quite good at gzip compression, often getting over 50% reductions in size
- Furthermore, WebAssembly's binary format is optimized for very fast parsing and processing

This means that [if you're using `instantiateStreaming`](https://hacks.mozilla.org/2018/01/making-webassembly-even-faster-firefoxs-new-streaming-and-tiering-compiler/) the second the Web request is done the WebAssembly module is probably ready to go

JavaScript, on the other hand, can often take longer to not only parse but also get up to speed with JIT compilation and such

## Optimizing Builds for Code Size

There are a bunch of configuration options we can use to get `rustc` to make smaller `.wasm` binaries

- Trading longer compile times for smaller `.wasm` sizes
- Can also trade runtime speed of the WebAssembly for smaller code size

### Compiling With Link Time Optimization (LTO)

In `Cargo.toml`, add `lto = true` in the `[profile.release]` section:

```toml
[profile.release]
lto = true
```

- Gives LLVM many more opportunities to inline and prune functions
- Not only will it make the `.wasm` smaller, but it will also make it faster at runtime
- The downside is that compilation will take longer

### Tell LLVM to Optimize for Size Instead of Speed

LLVM's optimization passes are tuned to improve speed, not size, by default

We can change the goal to code size by modifying the `[profile.release]` section in `Cargo.toml` to this:

```toml
[profile.release]
opt-level = 's'
```

Or, to even more aggressively optimize for size, at further potential speed costs:

```toml
[profile.release]
opt-level = 'z'
```

**Note:** Surprisingly enough, `opt-level = "s"` can sometimes result in smaller binaries than `opt-level = "z"`. Always measure!

## Use the `wasm-opt` Tool

The [Binaryen](https://github.com/WebAssembly/binaryen) toolkit is a collection of WebAssembly-specific compiler tools

- Goes much further than LLVM's WebAssembly backend does
- Using its `wasm-opt` tool to post-process a `.wasm` binary generated by LLVM can often get another 15-20% savings on code size
- It will often produce runtime speed ups at the same time!

```bash
# Optimize for size.
wasm-opt -Os -o output.wasm input.wasm

# Optimize aggressively for size.
wasm-opt -Oz -o output.wasm input.wasm

# Optimize for speed.
wasm-opt -O -o output.wasm input.wasm

# Optimize aggressively for speed.
wasm-opt -O3 -o output.wasm input.wasm
```

## Size Profiling

### **The `twiggy` Code Size Profiler**

`twiggy` is a code size profiler that supports WebAssembly as input. It analyzes a binary's call graph to answer questions like:

- Why was this function included in the binary in the first place?
- What is the *retained size* of this function? I.e. how much space would be saved if I removed it and all the functions that become dead code after its removal?

OR

```bash
ls -lh myfile.txt
```

## More invasive Tools and Techniques

### **Avoid String Formatting**

`format!`, `to_string`, etc... can bring in a lot of code bloat. If possible, only do string formatting in debug mode, and in release mode use static strings.

### Avoid Panicking

Panics do not always appear as a `panic!()` macro invocation. They arise implicitly from many constructs, such as:

- Indexing a slice panics on out of bounds indices: `my_slice[i]`
- Division will panic if the divisor is zero: `dividend / divisor`
- Unwrapping an `Option` or `Result`: `opt.unwrap()` or `res.unwrap()`

### Avoid Allocation or Switch to `wee_alloc`

Rust's default allocator for WebAssembly is a port of `dlmalloc` to Rust. It weighs in somewhere around ten kilobytes

- If you can completely avoid dynamic allocation, then you should be able to shed those ten kilobytes

Completely avoiding dynamic allocation can be very difficult. But removing allocation from hot code paths is usually much easier (and usually helps make those hot code paths faster, as well)

In these cases, replacing the default global allocator with `[wee_alloc](https://github.com/rustwasm/wee_alloc)` should save you most (but not quite all) of those ten kilobytes

- `wee_alloc` is an allocator designed for situations where you need *some* kind of allocator, but do not need a particularly fast allocator, and will happily trade allocation speed for smaller code size

```toml
[features]
default = ["wee_alloc"]

[dependencies]
wee_alloc = { version = "0.4.5", optional = true }
```

### Use Trait Objects Instead of Generic Parameters

When you create generic functions that use type parameters, like this:

```rust
fn whatever<T: MyTrait>(t: T) { ... }
```

Then `rustc` and LLVM will create a new copy of the function for each `T` type that the function is used with

- This presents many opportunities for compiler optimizations based on which particular `T` each copy is working with, but these copies add up quickly in terms of code size.

If you use trait objects instead of type parameters, like this:

```rust

fn whatever(t: Box<MyTrait>) { ... }
// or
fn whatever(t: &MyTrait) { ... }
// etc...
```

# In our Game of Life Example